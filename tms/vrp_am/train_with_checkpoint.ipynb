{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-13 13:04:49 model initialized\n",
      "2023-07-13 13:05:02 validation dataset created and saved on the disk\n",
      "Evaluating baseline model on baseline dataset (epoch = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rollout greedy execution: 100%|██████████| 10/10 [00:10<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-13 13:05:28 baseline initialized\n",
      "Current decode type: sampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch calculation at epoch 0: 1it [00:01,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_global_norm = 196.3567352294922, clipped_norm = 1.0000001192092896\n",
      "Epoch 0 (batch = 0): Loss: -0.7170138359069824: Cost: 15.783169746398926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch calculation at epoch 0: 23it [00:40,  1.76s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 58\u001B[0m\n\u001B[0;32m     47\u001B[0m baseline \u001B[38;5;241m=\u001B[39m RolloutBaseline(model_tf,\n\u001B[0;32m     48\u001B[0m                            wp_n_epochs \u001B[38;5;241m=\u001B[39m NUMBER_OF_WP_EPOCHS,\n\u001B[0;32m     49\u001B[0m                            epoch \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     54\u001B[0m                            graph_size\u001B[38;5;241m=\u001B[39mGRAPH_SIZE\n\u001B[0;32m     55\u001B[0m                            )\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28mprint\u001B[39m(get_cur_time(), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbaseline initialized\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 58\u001B[0m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     59\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmodel_tf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     60\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbaseline\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[43m            \u001B[49m\u001B[43mvalidation_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     62\u001B[0m \u001B[43m            \u001B[49m\u001B[43msamples\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mSAMPLES\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     63\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mBATCH\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     64\u001B[0m \u001B[43m            \u001B[49m\u001B[43mval_batch_size\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mVAL_BATCH_SIZE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     65\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstart_epoch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mSTART_EPOCH\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[43m            \u001B[49m\u001B[43mend_epoch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mEND_EPOCH\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     67\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfrom_checkpoint\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mFROM_CHECKPOINT\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     68\u001B[0m \u001B[43m            \u001B[49m\u001B[43mgrad_norm_clipping\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mGRAD_NORM_CLIPPING\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     69\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbatch_verbose\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mBATCH_VERBOSE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     70\u001B[0m \u001B[43m            \u001B[49m\u001B[43mgraph_size\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mGRAPH_SIZE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     71\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mFILENAME\u001B[49m\n\u001B[0;32m     72\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\bc\\algorithm\\tms\\vrp\\train.py:94\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(optimizer, model_tf, baseline, validation_dataset, samples, batch, val_batch_size, start_epoch, end_epoch, from_checkpoint, grad_norm_clipping, batch_verbose, graph_size, filename)\u001B[0m\n\u001B[0;32m     91\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m num_batch \u001B[38;5;241m%\u001B[39m batch_verbose \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     92\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgrad_global_norm = \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, clipped_norm = \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(init_global_norm\u001B[38;5;241m.\u001B[39mnumpy(), global_norm\u001B[38;5;241m.\u001B[39mnumpy()))\n\u001B[1;32m---> 94\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_gradients\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mzip\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_tf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainable_variables\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;66;03m# Track progress\u001B[39;00m\n\u001B[0;32m     97\u001B[0m epoch_loss_avg\u001B[38;5;241m.\u001B[39mupdate_state(loss_value)\n",
      "File \u001B[1;32mC:\\Program Files\\Python\\Python39\\lib\\site-packages\\keras\\optimizers\\optimizer.py:1174\u001B[0m, in \u001B[0;36mOptimizer.apply_gradients\u001B[1;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001B[0m\n\u001B[0;32m   1172\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m skip_gradients_aggregation \u001B[38;5;129;01mand\u001B[39;00m experimental_aggregate_gradients:\n\u001B[0;32m   1173\u001B[0m     grads_and_vars \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maggregate_gradients(grads_and_vars)\n\u001B[1;32m-> 1174\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_gradients\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrads_and_vars\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\Python\\Python39\\lib\\site-packages\\keras\\optimizers\\optimizer.py:650\u001B[0m, in \u001B[0;36m_BaseOptimizer.apply_gradients\u001B[1;34m(self, grads_and_vars, name)\u001B[0m\n\u001B[0;32m    648\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply_weight_decay(trainable_variables)\n\u001B[0;32m    649\u001B[0m grads_and_vars \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(grads, trainable_variables))\n\u001B[1;32m--> 650\u001B[0m iteration \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_internal_apply_gradients\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrads_and_vars\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    652\u001B[0m \u001B[38;5;66;03m# Apply variable constraints after applying gradients.\u001B[39;00m\n\u001B[0;32m    653\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m variable \u001B[38;5;129;01min\u001B[39;00m trainable_variables:\n",
      "File \u001B[1;32mC:\\Program Files\\Python\\Python39\\lib\\site-packages\\keras\\optimizers\\optimizer.py:1200\u001B[0m, in \u001B[0;36mOptimizer._internal_apply_gradients\u001B[1;34m(self, grads_and_vars)\u001B[0m\n\u001B[0;32m   1199\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_internal_apply_gradients\u001B[39m(\u001B[38;5;28mself\u001B[39m, grads_and_vars):\n\u001B[1;32m-> 1200\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__internal__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistribute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minterim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmaybe_merge_call\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1201\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_distributed_apply_gradients_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1202\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_distribution_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1203\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrads_and_vars\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1204\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\merge_call_interim.py:51\u001B[0m, in \u001B[0;36mmaybe_merge_call\u001B[1;34m(fn, strategy, *args, **kwargs)\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \n\u001B[0;32m     33\u001B[0m \u001B[38;5;124;03mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     48\u001B[0m \u001B[38;5;124;03m  The return value of the `fn` call.\u001B[39;00m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m strategy_supports_no_merge_call():\n\u001B[1;32m---> 51\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(strategy, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m distribution_strategy_context\u001B[38;5;241m.\u001B[39mget_replica_context()\u001B[38;5;241m.\u001B[39mmerge_call(\n\u001B[0;32m     54\u001B[0m       fn, args\u001B[38;5;241m=\u001B[39margs, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\Program Files\\Python\\Python39\\lib\\site-packages\\keras\\optimizers\\optimizer.py:1250\u001B[0m, in \u001B[0;36mOptimizer._distributed_apply_gradients_fn\u001B[1;34m(self, distribution, grads_and_vars, **kwargs)\u001B[0m\n\u001B[0;32m   1247\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_step(grad, var)\n\u001B[0;32m   1249\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m grad, var \u001B[38;5;129;01min\u001B[39;00m grads_and_vars:\n\u001B[1;32m-> 1250\u001B[0m     \u001B[43mdistribution\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextended\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1251\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvar\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mapply_grad_to_update_var\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mgrad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgroup\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[0;32m   1252\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1254\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_ema:\n\u001B[0;32m   1255\u001B[0m     _, var_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mgrads_and_vars)\n",
      "File \u001B[1;32mC:\\Program Files\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2637\u001B[0m, in \u001B[0;36mStrategyExtendedV2.update\u001B[1;34m(self, var, fn, args, kwargs, group)\u001B[0m\n\u001B[0;32m   2634\u001B[0m   fn \u001B[38;5;241m=\u001B[39m autograph\u001B[38;5;241m.\u001B[39mtf_convert(\n\u001B[0;32m   2635\u001B[0m       fn, autograph_ctx\u001B[38;5;241m.\u001B[39mcontrol_status_ctx(), convert_by_default\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m   2636\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_container_strategy()\u001B[38;5;241m.\u001B[39mscope():\n\u001B[1;32m-> 2637\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_update\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvar\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2638\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2639\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_replica_ctx_update(\n\u001B[0;32m   2640\u001B[0m       var, fn, args\u001B[38;5;241m=\u001B[39margs, kwargs\u001B[38;5;241m=\u001B[39mkwargs, group\u001B[38;5;241m=\u001B[39mgroup)\n",
      "File \u001B[1;32mC:\\Program Files\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3710\u001B[0m, in \u001B[0;36m_DefaultDistributionExtended._update\u001B[1;34m(self, var, fn, args, kwargs, group)\u001B[0m\n\u001B[0;32m   3707\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_update\u001B[39m(\u001B[38;5;28mself\u001B[39m, var, fn, args, kwargs, group):\n\u001B[0;32m   3708\u001B[0m   \u001B[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001B[39;00m\n\u001B[0;32m   3709\u001B[0m   \u001B[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001B[39;00m\n\u001B[1;32m-> 3710\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_update_non_slot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvar\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mvar\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3716\u001B[0m, in \u001B[0;36m_DefaultDistributionExtended._update_non_slot\u001B[1;34m(self, colocate_with, fn, args, kwargs, should_group)\u001B[0m\n\u001B[0;32m   3712\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_update_non_slot\u001B[39m(\u001B[38;5;28mself\u001B[39m, colocate_with, fn, args, kwargs, should_group):\n\u001B[0;32m   3713\u001B[0m   \u001B[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001B[39;00m\n\u001B[0;32m   3714\u001B[0m   \u001B[38;5;66;03m# once that value is used for something.\u001B[39;00m\n\u001B[0;32m   3715\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m UpdateContext(colocate_with):\n\u001B[1;32m-> 3716\u001B[0m     result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   3717\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m should_group:\n\u001B[0;32m   3718\u001B[0m       \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32mC:\\Program Files\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:595\u001B[0m, in \u001B[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    593\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    594\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m ag_ctx\u001B[38;5;241m.\u001B[39mControlStatusCtx(status\u001B[38;5;241m=\u001B[39mag_ctx\u001B[38;5;241m.\u001B[39mStatus\u001B[38;5;241m.\u001B[39mUNSPECIFIED):\n\u001B[1;32m--> 595\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\Program Files\\Python\\Python39\\lib\\site-packages\\keras\\optimizers\\optimizer.py:1247\u001B[0m, in \u001B[0;36mOptimizer._distributed_apply_gradients_fn.<locals>.apply_grad_to_update_var\u001B[1;34m(var, grad)\u001B[0m\n\u001B[0;32m   1245\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_step_xla(grad, var, \u001B[38;5;28mid\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_var_key(var)))\n\u001B[0;32m   1246\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1247\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_update_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvar\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\Python\\Python39\\lib\\site-packages\\keras\\optimizers\\optimizer.py:240\u001B[0m, in \u001B[0;36m_BaseOptimizer._update_step\u001B[1;34m(self, gradient, variable)\u001B[0m\n\u001B[0;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_var_key(variable) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_index_dict:\n\u001B[0;32m    232\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\n\u001B[0;32m    233\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe optimizer cannot recognize variable \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvariable\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    234\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis usually means you are trying to call the optimizer to \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    238\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`tf.keras.optimizers.legacy.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    239\u001B[0m     )\n\u001B[1;32m--> 240\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvariable\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\Python\\Python39\\lib\\site-packages\\keras\\optimizers\\adam.py:200\u001B[0m, in \u001B[0;36mAdam.update_step\u001B[1;34m(self, gradient, variable)\u001B[0m\n\u001B[0;32m    198\u001B[0m     v_hat\u001B[38;5;241m.\u001B[39massign(tf\u001B[38;5;241m.\u001B[39mmaximum(v_hat, v))\n\u001B[0;32m    199\u001B[0m     v \u001B[38;5;241m=\u001B[39m v_hat\n\u001B[1;32m--> 200\u001B[0m \u001B[43mvariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43massign_sub\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mm\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43malpha\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqrt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepsilon\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:912\u001B[0m, in \u001B[0;36mBaseResourceVariable.assign_sub\u001B[1;34m(self, delta, use_locking, name, read_value)\u001B[0m\n\u001B[0;32m    908\u001B[0m \u001B[38;5;66;03m# TODO(apassos): this here and below is not atomic. Consider making it\u001B[39;00m\n\u001B[0;32m    909\u001B[0m \u001B[38;5;66;03m# atomic if there's a way to do so without a performance cost for those who\u001B[39;00m\n\u001B[0;32m    910\u001B[0m \u001B[38;5;66;03m# don't need it.\u001B[39;00m\n\u001B[0;32m    911\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _handle_graph(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_assign_dependencies():\n\u001B[1;32m--> 912\u001B[0m   assign_sub_op \u001B[38;5;241m=\u001B[39m \u001B[43mgen_resource_variable_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43massign_sub_variable_op\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    913\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    914\u001B[0m \u001B[43m      \u001B[49m\u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_to_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdelta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    915\u001B[0m \u001B[43m      \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    916\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m read_value:\n\u001B[0;32m    917\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lazy_read(assign_sub_op)\n",
      "File \u001B[1;32mC:\\Program Files\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:93\u001B[0m, in \u001B[0;36massign_sub_variable_op\u001B[1;34m(resource, value, name)\u001B[0m\n\u001B[0;32m     91\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tld\u001B[38;5;241m.\u001B[39mis_eager:\n\u001B[0;32m     92\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 93\u001B[0m     _result \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_FastPathExecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     94\u001B[0m \u001B[43m      \u001B[49m\u001B[43m_ctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mAssignSubVariableOp\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     95\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _result\n\u001B[0;32m     96\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m _core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from time import gmtime, strftime\n",
    "\n",
    "from attention_model import AttentionModel, set_decode_type\n",
    "from reinforce_baseline import RolloutBaseline\n",
    "from train import train_model\n",
    "\n",
    "from utils import create_data_on_disk, get_cur_time\n",
    "\n",
    "\n",
    "# Params of model\n",
    "SAMPLES = 128000 # 512*250\n",
    "BATCH = 512\n",
    "START_EPOCH = 0\n",
    "END_EPOCH = 5\n",
    "FROM_CHECKPOINT = False\n",
    "embedding_dim = 128\n",
    "LEARNING_RATE = 0.0001\n",
    "ROLLOUT_SAMPLES = 10000\n",
    "NUMBER_OF_WP_EPOCHS = 1\n",
    "GRAD_NORM_CLIPPING = 1.0\n",
    "BATCH_VERBOSE = 1000\n",
    "VAL_BATCH_SIZE = 1000\n",
    "VALIDATE_SET_SIZE = 10000\n",
    "SEED = 1234\n",
    "GRAPH_SIZE = 20\n",
    "FILENAME = 'VRP_{}_{}'.format(GRAPH_SIZE, strftime(\"%Y-%m-%d\", gmtime()))\n",
    "\n",
    "# Initialize model\n",
    "model_tf = AttentionModel(embedding_dim)\n",
    "set_decode_type(model_tf, \"sampling\")\n",
    "print(get_cur_time(), 'model initialized')\n",
    "\n",
    "# Create and save validation dataset\n",
    "validation_dataset = create_data_on_disk(GRAPH_SIZE,\n",
    "                                         VALIDATE_SET_SIZE,\n",
    "                                         is_save=True,\n",
    "                                         filename=FILENAME,\n",
    "                                         is_return=True,\n",
    "                                         seed = SEED)\n",
    "print(get_cur_time(), 'validation dataset created and saved on the disk')\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
    "\n",
    "# Initialize baseline\n",
    "baseline = RolloutBaseline(model_tf,\n",
    "                           wp_n_epochs = NUMBER_OF_WP_EPOCHS,\n",
    "                           epoch = 0,\n",
    "                           num_samples=ROLLOUT_SAMPLES,\n",
    "                           filename = FILENAME,\n",
    "                           from_checkpoint = FROM_CHECKPOINT,\n",
    "                           embedding_dim=embedding_dim,\n",
    "                           graph_size=GRAPH_SIZE\n",
    "                           )\n",
    "print(get_cur_time(), 'baseline initialized')\n",
    "\n",
    "train_model(optimizer,\n",
    "            model_tf,\n",
    "            baseline,\n",
    "            validation_dataset,\n",
    "            samples = SAMPLES,\n",
    "            batch = BATCH,\n",
    "            val_batch_size = VAL_BATCH_SIZE,\n",
    "            start_epoch = START_EPOCH,\n",
    "            end_epoch = END_EPOCH,\n",
    "            from_checkpoint = FROM_CHECKPOINT,\n",
    "            grad_norm_clipping = GRAD_NORM_CLIPPING,\n",
    "            batch_verbose = BATCH_VERBOSE,\n",
    "            graph_size = GRAPH_SIZE,\n",
    "            filename = FILENAME\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Weight count mismatch for layer #0 (named graph_attention_encoder_1 in the current model, graph_attention_encoder in the save file). Layer expects 36 weight(s). Received 52 saved weight(s)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 35\u001B[0m\n\u001B[0;32m     32\u001B[0m BASELINE_MODEL_PATH \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbaseline_checkpoint_epoch_4_VRP_20_2023-07-13.h5\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;66;03m# Initialize model\u001B[39;00m\n\u001B[1;32m---> 35\u001B[0m model_tf \u001B[38;5;241m=\u001B[39m \u001B[43mload_tf_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mMODEL_PATH\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     36\u001B[0m \u001B[43m                         \u001B[49m\u001B[43membedding_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43membedding_dim\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     37\u001B[0m \u001B[43m                         \u001B[49m\u001B[43mgraph_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mGRAPH_SIZE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     38\u001B[0m set_decode_type(model_tf, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msampling\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28mprint\u001B[39m(get_cur_time(), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel loaded\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32mD:\\bc\\algorithm\\tms\\vrp\\reinforce_baseline.py:221\u001B[0m, in \u001B[0;36mload_tf_model\u001B[1;34m(path, embedding_dim, graph_size, n_encode_layers)\u001B[0m\n\u001B[0;32m    218\u001B[0m set_decode_type(model_loaded, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgreedy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    219\u001B[0m _, _ \u001B[38;5;241m=\u001B[39m model_loaded(data_random)\n\u001B[1;32m--> 221\u001B[0m \u001B[43mmodel_loaded\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_weights\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    223\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m model_loaded\n",
      "File \u001B[1;32mC:\\Program Files\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32mC:\\Program Files\\Python\\Python39\\lib\\site-packages\\keras\\saving\\legacy\\hdf5_format.py:826\u001B[0m, in \u001B[0;36mload_weights_from_hdf5_group\u001B[1;34m(f, model)\u001B[0m\n\u001B[0;32m    822\u001B[0m     weight_values \u001B[38;5;241m=\u001B[39m preprocess_weights_for_loading(\n\u001B[0;32m    823\u001B[0m         layer, weight_values, original_keras_version, original_backend\n\u001B[0;32m    824\u001B[0m     )\n\u001B[0;32m    825\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(weight_values) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(symbolic_weights):\n\u001B[1;32m--> 826\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    827\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWeight count mismatch for layer #\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (named \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlayer\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m in \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    828\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthe current model, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m in the save file). \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    829\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLayer expects \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(symbolic_weights)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m weight(s). Received \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    830\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(weight_values)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m saved weight(s)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    831\u001B[0m         )\n\u001B[0;32m    832\u001B[0m     weight_value_tuples \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mzip\u001B[39m(symbolic_weights, weight_values)\n\u001B[0;32m    834\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtop_level_model_weights\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m f:\n",
      "\u001B[1;31mValueError\u001B[0m: Weight count mismatch for layer #0 (named graph_attention_encoder_1 in the current model, graph_attention_encoder in the save file). Layer expects 36 weight(s). Received 52 saved weight(s)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from time import gmtime, strftime\n",
    "\n",
    "from attention_model import set_decode_type\n",
    "from reinforce_baseline import RolloutBaseline\n",
    "from train import train_model\n",
    "\n",
    "from utils import get_cur_time\n",
    "from reinforce_baseline import load_tf_model\n",
    "from utils import read_from_pickle\n",
    "\n",
    "\n",
    "SAMPLES = 128000 # 512*250\n",
    "BATCH = 512\n",
    "LEARNING_RATE = 0.0001\n",
    "ROLLOUT_SAMPLES = 10000\n",
    "NUMBER_OF_WP_EPOCHS = 1\n",
    "GRAD_NORM_CLIPPING = 1.0\n",
    "BATCH_VERBOSE = 1000\n",
    "VAL_BATCH_SIZE = 1000\n",
    "VALIDATE_SET_SIZE = 10000\n",
    "SEED = 1234\n",
    "GRAPH_SIZE = 20\n",
    "FILENAME = 'VRP_{}_{}'.format(GRAPH_SIZE, strftime(\"%Y-%m-%d\", gmtime()))\n",
    "\n",
    "START_EPOCH = 5\n",
    "END_EPOCH = 10\n",
    "FROM_CHECKPOINT = True\n",
    "embedding_dim = 128\n",
    "MODEL_PATH = 'model_checkpoint_epoch_4_VRP_20_2023-07-13.h5'\n",
    "VAL_SET_PATH = 'Validation_dataset_VRP_20_2023-07-13.pkl'\n",
    "BASELINE_MODEL_PATH = 'baseline_checkpoint_epoch_4_VRP_20_2023-07-13.h5'\n",
    "\n",
    "# Initialize model\n",
    "model_tf = load_tf_model(MODEL_PATH,\n",
    "                         embedding_dim=embedding_dim,\n",
    "                         graph_size=GRAPH_SIZE)\n",
    "set_decode_type(model_tf, \"sampling\")\n",
    "print(get_cur_time(), 'model loaded')\n",
    "\n",
    "# Create and save validation dataset\n",
    "validation_dataset = read_from_pickle(VAL_SET_PATH)\n",
    "print(get_cur_time(), 'validation dataset loaded')\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
    "\n",
    "# Initialize baseline\n",
    "baseline = RolloutBaseline(model_tf,\n",
    "                           wp_n_epochs = NUMBER_OF_WP_EPOCHS,\n",
    "                           epoch = START_EPOCH,\n",
    "                           num_samples=ROLLOUT_SAMPLES,\n",
    "                           filename = FILENAME,\n",
    "                           from_checkpoint = FROM_CHECKPOINT,\n",
    "                           embedding_dim=embedding_dim,\n",
    "                           graph_size=GRAPH_SIZE,\n",
    "                           path_to_checkpoint = BASELINE_MODEL_PATH)\n",
    "print(get_cur_time(), 'baseline initialized')\n",
    "\n",
    "train_model(optimizer,\n",
    "            model_tf,\n",
    "            baseline,\n",
    "            validation_dataset,\n",
    "            samples = SAMPLES,\n",
    "            batch = BATCH,\n",
    "            val_batch_size = VAL_BATCH_SIZE,\n",
    "            start_epoch = START_EPOCH,\n",
    "            end_epoch = END_EPOCH,\n",
    "            from_checkpoint = FROM_CHECKPOINT,\n",
    "            grad_norm_clipping = GRAD_NORM_CLIPPING,\n",
    "            batch_verbose = BATCH_VERBOSE,\n",
    "            graph_size = GRAPH_SIZE,\n",
    "            filename = FILENAME\n",
    "            )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
